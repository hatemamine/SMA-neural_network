{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapsulNet10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPSFkRqgEdbz",
        "colab_type": "code",
        "outputId": "be382ebe-f9a5-4dd6-fb57-bb1eac91f76b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5664
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "from keras import activations\n",
        "from keras import utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler ,ReduceLROnPlateau\n",
        "import numpy as np\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-2\n",
        "    if epoch > 50:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 40:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 30:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 20:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "# the squashing function.\n",
        "# we use 0.5 in stead of 1 in hinton's paper.\n",
        "# if 1, the norm of vector will be zoomed out.\n",
        "# if 0.5, the norm will be zoomed in while original norm is less than 0.5\n",
        "# and be zoomed out while original norm is greater than 0.5.\n",
        "def squash(x, axis=-1):\n",
        "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
        "    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n",
        "    return scale * x\n",
        "\n",
        "\n",
        "# define our own softmax function instead of K.softmax\n",
        "# because K.softmax can not specify axis.\n",
        "def softmax(x, axis=-1):\n",
        "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "    return ex / K.sum(ex, axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "# define the margin loss like hinge loss\n",
        "def margin_loss(y_true, y_pred):\n",
        "    lamb, margin = 0.5, 0.1\n",
        "    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n",
        "        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n",
        "\n",
        "\n",
        "class Capsule(Layer):\n",
        "    \"\"\"A Capsule Implement with Pure Keras\n",
        "    There are two vesions of Capsule.\n",
        "    One is like dense layer (for the fixed-shape input),\n",
        "    and the other is like timedistributed dense (for various length input).\n",
        "\n",
        "    The input shape of Capsule must be (batch_size,\n",
        "                                        input_num_capsule,\n",
        "                                        input_dim_capsule\n",
        "                                       )\n",
        "    and the output shape is (batch_size,\n",
        "                             num_capsule,\n",
        "                             dim_capsule\n",
        "                            )\n",
        "\n",
        "    Capsule Implement is from https://github.com/bojone/Capsule/\n",
        "    Capsule Paper: https://arxiv.org/abs/1710.09829\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_capsule,\n",
        "                 dim_capsule,\n",
        "                 routings=3,\n",
        "                 share_weights=True,\n",
        "                 activation='squash',\n",
        "                 **kwargs):\n",
        "        super(Capsule, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.share_weights = share_weights\n",
        "        if activation == 'squash':\n",
        "            self.activation = squash\n",
        "        else:\n",
        "            self.activation = activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim_capsule = input_shape[-1]\n",
        "        if self.share_weights:\n",
        "            self.kernel = self.add_weight(\n",
        "                name='capsule_kernel',\n",
        "                shape=(1, input_dim_capsule,\n",
        "                       self.num_capsule * self.dim_capsule),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True)\n",
        "        else:\n",
        "            input_num_capsule = input_shape[-2]\n",
        "            self.kernel = self.add_weight(\n",
        "                name='capsule_kernel',\n",
        "                shape=(input_num_capsule, input_dim_capsule,\n",
        "                       self.num_capsule * self.dim_capsule),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Following the routing algorithm from Hinton's paper,\n",
        "        but replace b = b + <u,v> with b = <u,v>.\n",
        "\n",
        "        This change can improve the feature representation of Capsule.\n",
        "\n",
        "        However, you can replace\n",
        "            b = K.batch_dot(outputs, hat_inputs, [2, 3])\n",
        "        with\n",
        "            b += K.batch_dot(outputs, hat_inputs, [2, 3])\n",
        "        to realize a standard routing.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.share_weights:\n",
        "            hat_inputs = K.conv1d(inputs, self.kernel)\n",
        "        else:\n",
        "            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n",
        "\n",
        "        batch_size = K.shape(inputs)[0]\n",
        "        input_num_capsule = K.shape(inputs)[1]\n",
        "        hat_inputs = K.reshape(hat_inputs,\n",
        "                               (batch_size, input_num_capsule,\n",
        "                                self.num_capsule, self.dim_capsule))\n",
        "        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n",
        "\n",
        "        b = K.zeros_like(hat_inputs[:, :, :, 0])\n",
        "        for i in range(self.routings):\n",
        "            c = softmax(b, 1)\n",
        "            o = self.activation(K.batch_dot(c, hat_inputs, [2, 2]))\n",
        "            if i < self.routings - 1:\n",
        "                b = K.batch_dot(o, hat_inputs, [2, 3])\n",
        "                if K.backend() == 'theano':\n",
        "                    o = K.sum(o, axis=1)\n",
        "\n",
        "        return o\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, self.num_capsule, self.dim_capsule)\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train = utils.to_categorical(y_train, num_classes)\n",
        "y_test = utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# A common Conv2D model\n",
        "input_image = Input(shape=(None, None, 3))\n",
        "x = Conv2D(64, (3, 3), activation='relu')(input_image)\n",
        "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "x = AveragePooling2D((2, 2))(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
        "\n",
        "\n",
        "\"\"\"now we reshape it as (batch_size, input_num_capsule, input_dim_capsule)\n",
        "then connect a Capsule layer.\n",
        "\n",
        "the output of final model is the lengths of 10 Capsule, whose dim=16.\n",
        "\n",
        "the length of Capsule is the proba,\n",
        "so the problem becomes a 10 two-classification problem.\n",
        "\"\"\"\n",
        "\n",
        "x = Reshape((-1, 128))(x)\n",
        "capsule = Capsule(10, 16, 3, True)(x)\n",
        "output = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule)\n",
        "model = Model(inputs=input_image, outputs=output)\n",
        "\n",
        "# we use a margin loss\n",
        "model.compile(loss=margin_loss, optimizer=Adam(lr=lr_schedule(0)), metrics=['accuracy'])\n",
        "model.summary()\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [lr_reducer, lr_scheduler ]\n",
        "# we can compare the performance with or without data augmentation\n",
        "data_augmentation = True\n",
        "\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(x_test, y_test),\n",
        "        shuffle=True , callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by dataset std\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in 0 to 180 degrees\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally\n",
        "        height_shift_range=0.1,  # randomly shift images vertically\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    model.fit_generator(\n",
        "        datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "        epochs=epochs,\n",
        "        validation_data=(x_test, y_test),\n",
        "        callbacks=callbacks , steps_per_epoch=391)    "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate:  0.001\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         (None, None, None, 3)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "average_pooling2d_7 (Average (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "reshape_7 (Reshape)          (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "capsule_7 (Capsule)          (None, 10, 16)            20480     \n",
            "_________________________________________________________________\n",
            "lambda_7 (Lambda)            (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 280,640\n",
            "Trainable params: 280,640\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 104s 266ms/step - loss: 0.4381 - acc: 0.3173 - val_loss: 0.3720 - val_acc: 0.4452\n",
            "Epoch 2/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 89s 228ms/step - loss: 0.3491 - acc: 0.4930 - val_loss: 0.3265 - val_acc: 0.5248\n",
            "Epoch 3/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 98s 251ms/step - loss: 0.3065 - acc: 0.5665 - val_loss: 0.3052 - val_acc: 0.5561\n",
            "Epoch 4/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 94s 240ms/step - loss: 0.2755 - acc: 0.6194 - val_loss: 0.2535 - val_acc: 0.6584\n",
            "Epoch 5/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 101s 258ms/step - loss: 0.2532 - acc: 0.6554 - val_loss: 0.2492 - val_acc: 0.6606\n",
            "Epoch 6/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.2334 - acc: 0.6866 - val_loss: 0.2214 - val_acc: 0.7059\n",
            "Epoch 7/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 100s 255ms/step - loss: 0.2196 - acc: 0.7093 - val_loss: 0.2132 - val_acc: 0.7233\n",
            "Epoch 8/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 98s 250ms/step - loss: 0.2096 - acc: 0.7266 - val_loss: 0.2036 - val_acc: 0.7384\n",
            "Epoch 9/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.2005 - acc: 0.7395 - val_loss: 0.1943 - val_acc: 0.7491\n",
            "Epoch 10/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 95s 243ms/step - loss: 0.1920 - acc: 0.7537 - val_loss: 0.1922 - val_acc: 0.7557\n",
            "Epoch 11/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 95s 242ms/step - loss: 0.1865 - acc: 0.7623 - val_loss: 0.1878 - val_acc: 0.7535\n",
            "Epoch 12/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 98s 251ms/step - loss: 0.1797 - acc: 0.7729 - val_loss: 0.1893 - val_acc: 0.7641\n",
            "Epoch 13/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1739 - acc: 0.7803 - val_loss: 0.1757 - val_acc: 0.7766\n",
            "Epoch 14/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 97s 248ms/step - loss: 0.1694 - acc: 0.7902 - val_loss: 0.1793 - val_acc: 0.7672\n",
            "Epoch 15/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 92s 234ms/step - loss: 0.1668 - acc: 0.7940 - val_loss: 0.1724 - val_acc: 0.7799\n",
            "Epoch 16/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 97s 249ms/step - loss: 0.1625 - acc: 0.8012 - val_loss: 0.1902 - val_acc: 0.7659\n",
            "Epoch 17/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 92s 236ms/step - loss: 0.1579 - acc: 0.8080 - val_loss: 0.1772 - val_acc: 0.7724\n",
            "Epoch 18/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 96s 245ms/step - loss: 0.1552 - acc: 0.8128 - val_loss: 0.1734 - val_acc: 0.7838\n",
            "Epoch 19/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 89s 229ms/step - loss: 0.1511 - acc: 0.8189 - val_loss: 0.1746 - val_acc: 0.7788\n",
            "Epoch 20/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 100s 255ms/step - loss: 0.1489 - acc: 0.8224 - val_loss: 0.1634 - val_acc: 0.7974\n",
            "Epoch 21/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 92s 236ms/step - loss: 0.1462 - acc: 0.8261 - val_loss: 0.1650 - val_acc: 0.7985\n",
            "Epoch 22/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 94s 241ms/step - loss: 0.1429 - acc: 0.8318 - val_loss: 0.1618 - val_acc: 0.7973\n",
            "Epoch 23/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 229ms/step - loss: 0.1405 - acc: 0.8362 - val_loss: 0.1634 - val_acc: 0.7958\n",
            "Epoch 24/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 101s 258ms/step - loss: 0.1376 - acc: 0.8394 - val_loss: 0.1639 - val_acc: 0.8000\n",
            "Epoch 25/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 92s 236ms/step - loss: 0.1364 - acc: 0.8409 - val_loss: 0.1729 - val_acc: 0.7916\n",
            "Epoch 26/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 94s 241ms/step - loss: 0.1337 - acc: 0.8452 - val_loss: 0.1582 - val_acc: 0.8028\n",
            "Epoch 27/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 239ms/step - loss: 0.1327 - acc: 0.8450 - val_loss: 0.1647 - val_acc: 0.7927\n",
            "Epoch 28/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 94s 241ms/step - loss: 0.1312 - acc: 0.8498 - val_loss: 0.1586 - val_acc: 0.8047\n",
            "Epoch 29/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 94s 239ms/step - loss: 0.1280 - acc: 0.8533 - val_loss: 0.1591 - val_acc: 0.8105\n",
            "Epoch 30/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 92s 235ms/step - loss: 0.1272 - acc: 0.8558 - val_loss: 0.1709 - val_acc: 0.7879\n",
            "Epoch 31/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 96s 245ms/step - loss: 0.1259 - acc: 0.8569 - val_loss: 0.1559 - val_acc: 0.8088\n",
            "Epoch 32/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 88s 226ms/step - loss: 0.1242 - acc: 0.8594 - val_loss: 0.1588 - val_acc: 0.8040\n",
            "Epoch 33/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.1227 - acc: 0.8630 - val_loss: 0.1589 - val_acc: 0.8024\n",
            "Epoch 34/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1213 - acc: 0.8651 - val_loss: 0.1572 - val_acc: 0.8035\n",
            "Epoch 35/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 95s 243ms/step - loss: 0.1204 - acc: 0.8659 - val_loss: 0.1527 - val_acc: 0.8124\n",
            "Epoch 36/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 94s 240ms/step - loss: 0.1187 - acc: 0.8708 - val_loss: 0.1542 - val_acc: 0.8108\n",
            "Epoch 37/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 96s 245ms/step - loss: 0.1173 - acc: 0.8714 - val_loss: 0.1550 - val_acc: 0.8074\n",
            "Epoch 38/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 231ms/step - loss: 0.1161 - acc: 0.8738 - val_loss: 0.1502 - val_acc: 0.8177\n",
            "Epoch 39/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 94s 241ms/step - loss: 0.1153 - acc: 0.8745 - val_loss: 0.1598 - val_acc: 0.8122\n",
            "Epoch 40/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 88s 225ms/step - loss: 0.1137 - acc: 0.8769 - val_loss: 0.1534 - val_acc: 0.8190\n",
            "Epoch 41/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 99s 254ms/step - loss: 0.1131 - acc: 0.8777 - val_loss: 0.1485 - val_acc: 0.8178\n",
            "Epoch 42/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 92s 236ms/step - loss: 0.1109 - acc: 0.8807 - val_loss: 0.1473 - val_acc: 0.8255\n",
            "Epoch 43/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 92s 235ms/step - loss: 0.1107 - acc: 0.8807 - val_loss: 0.1505 - val_acc: 0.8196\n",
            "Epoch 44/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 98s 251ms/step - loss: 0.1091 - acc: 0.8847 - val_loss: 0.1522 - val_acc: 0.8148\n",
            "Epoch 45/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 98s 250ms/step - loss: 0.1089 - acc: 0.8840 - val_loss: 0.1537 - val_acc: 0.8108\n",
            "Epoch 46/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 231ms/step - loss: 0.1065 - acc: 0.8891 - val_loss: 0.1466 - val_acc: 0.8256\n",
            "Epoch 47/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 97s 247ms/step - loss: 0.1061 - acc: 0.8878 - val_loss: 0.1471 - val_acc: 0.8219\n",
            "Epoch 48/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 91s 233ms/step - loss: 0.1064 - acc: 0.8892 - val_loss: 0.1476 - val_acc: 0.8250\n",
            "Epoch 49/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 97s 247ms/step - loss: 0.1047 - acc: 0.8909 - val_loss: 0.1516 - val_acc: 0.8149\n",
            "Epoch 50/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 231ms/step - loss: 0.1031 - acc: 0.8934 - val_loss: 0.1531 - val_acc: 0.8160\n",
            "Epoch 51/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 97s 248ms/step - loss: 0.1028 - acc: 0.8943 - val_loss: 0.1546 - val_acc: 0.8173\n",
            "Epoch 52/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 96s 245ms/step - loss: 0.1022 - acc: 0.8948 - val_loss: 0.1517 - val_acc: 0.8113\n",
            "Epoch 53/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 97s 247ms/step - loss: 0.1013 - acc: 0.8951 - val_loss: 0.1519 - val_acc: 0.8206\n",
            "Epoch 54/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 94s 240ms/step - loss: 0.1002 - acc: 0.8980 - val_loss: 0.1523 - val_acc: 0.8196\n",
            "Epoch 55/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1001 - acc: 0.8994 - val_loss: 0.1534 - val_acc: 0.8188\n",
            "Epoch 56/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.0988 - acc: 0.8983 - val_loss: 0.1485 - val_acc: 0.8251\n",
            "Epoch 57/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.0982 - acc: 0.9010 - val_loss: 0.1463 - val_acc: 0.8281\n",
            "Epoch 58/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 96s 246ms/step - loss: 0.0971 - acc: 0.9023 - val_loss: 0.1487 - val_acc: 0.8246\n",
            "Epoch 59/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 89s 228ms/step - loss: 0.0967 - acc: 0.9023 - val_loss: 0.1531 - val_acc: 0.8216\n",
            "Epoch 60/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 97s 247ms/step - loss: 0.0970 - acc: 0.9018 - val_loss: 0.1553 - val_acc: 0.8120\n",
            "Epoch 61/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 88s 225ms/step - loss: 0.0956 - acc: 0.9050 - val_loss: 0.1514 - val_acc: 0.8192\n",
            "Epoch 62/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 95s 243ms/step - loss: 0.0945 - acc: 0.9080 - val_loss: 0.1521 - val_acc: 0.8177\n",
            "Epoch 63/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.0949 - acc: 0.9062 - val_loss: 0.1565 - val_acc: 0.8192\n",
            "Epoch 64/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 92s 235ms/step - loss: 0.0937 - acc: 0.9082 - val_loss: 0.1560 - val_acc: 0.8202\n",
            "Epoch 65/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 94s 240ms/step - loss: 0.0928 - acc: 0.9085 - val_loss: 0.1575 - val_acc: 0.8121\n",
            "Epoch 66/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 98s 250ms/step - loss: 0.0921 - acc: 0.9092 - val_loss: 0.1496 - val_acc: 0.8261\n",
            "Epoch 67/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 92s 234ms/step - loss: 0.0915 - acc: 0.9099 - val_loss: 0.1523 - val_acc: 0.8228\n",
            "Epoch 68/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 97s 247ms/step - loss: 0.0909 - acc: 0.9106 - val_loss: 0.1495 - val_acc: 0.8201\n",
            "Epoch 69/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 109s 278ms/step - loss: 0.0908 - acc: 0.9115 - val_loss: 0.1467 - val_acc: 0.8317\n",
            "Epoch 70/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 100s 257ms/step - loss: 0.0908 - acc: 0.9118 - val_loss: 0.1538 - val_acc: 0.8154\n",
            "Epoch 71/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 97s 249ms/step - loss: 0.0892 - acc: 0.9138 - val_loss: 0.1511 - val_acc: 0.8213\n",
            "Epoch 72/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 117s 299ms/step - loss: 0.0890 - acc: 0.9130 - val_loss: 0.1507 - val_acc: 0.8254\n",
            "Epoch 73/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 100s 255ms/step - loss: 0.0880 - acc: 0.9156 - val_loss: 0.1514 - val_acc: 0.8235\n",
            "Epoch 74/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.0878 - acc: 0.9167 - val_loss: 0.1502 - val_acc: 0.8266\n",
            "Epoch 75/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 100s 256ms/step - loss: 0.0875 - acc: 0.9172 - val_loss: 0.1495 - val_acc: 0.8209\n",
            "Epoch 76/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.0871 - acc: 0.9177 - val_loss: 0.1513 - val_acc: 0.8256\n",
            "Epoch 77/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 96s 246ms/step - loss: 0.0866 - acc: 0.9187 - val_loss: 0.1478 - val_acc: 0.8273\n",
            "Epoch 78/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 91s 233ms/step - loss: 0.0867 - acc: 0.9183 - val_loss: 0.1531 - val_acc: 0.8246\n",
            "Epoch 79/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 118s 302ms/step - loss: 0.0862 - acc: 0.9182 - val_loss: 0.1527 - val_acc: 0.8217\n",
            "Epoch 80/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 105s 268ms/step - loss: 0.0851 - acc: 0.9217 - val_loss: 0.1501 - val_acc: 0.8235\n",
            "Epoch 81/200\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 109s 279ms/step - loss: 0.0839 - acc: 0.9226 - val_loss: 0.1528 - val_acc: 0.8248\n",
            "Epoch 82/200\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 102s 261ms/step - loss: 0.0698 - acc: 0.9413 - val_loss: 0.1439 - val_acc: 0.8305\n",
            "Epoch 83/200\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 109s 280ms/step - loss: 0.0668 - acc: 0.9469 - val_loss: 0.1437 - val_acc: 0.8326\n",
            "Epoch 84/200\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 99s 253ms/step - loss: 0.0659 - acc: 0.9474 - val_loss: 0.1431 - val_acc: 0.8345\n",
            "Epoch 85/200\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 141s 360ms/step - loss: 0.0652 - acc: 0.9485 - val_loss: 0.1429 - val_acc: 0.8349\n",
            "Epoch 86/200\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 115s 295ms/step - loss: 0.0647 - acc: 0.9491 - val_loss: 0.1444 - val_acc: 0.8312\n",
            "Epoch 87/200\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 101s 257ms/step - loss: 0.0648 - acc: 0.9494 - val_loss: 0.1430 - val_acc: 0.8338\n",
            "Epoch 88/200\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.0639 - acc: 0.9501 - val_loss: 0.1437 - val_acc: 0.8332\n",
            "Epoch 89/200\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 115s 295ms/step - loss: 0.0636 - acc: 0.9510 - val_loss: 0.1430 - val_acc: 0.8346\n",
            "Epoch 90/200\n",
            "Learning rate:  0.0001\n",
            "391/391 [==============================] - 107s 273ms/step - loss: 0.0632 - acc: 0.9509 - val_loss: 0.1434 - val_acc: 0.8344\n",
            "Epoch 91/200\n",
            "Learning rate:  0.0001\n",
            "106/391 [=======>......................] - ETA: 1:10 - loss: 0.0631 - acc: 0.9528"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-81078f5ce3c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         callbacks=callbacks , steps_per_epoch=391)    \n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0menqueuer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 \u001b[0menqueuer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mval_enqueuer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfinished_tasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}